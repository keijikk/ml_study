{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_conv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keijikk/ml_study/blob/master/03_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wpIyoh50Gwmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!apt-get install lshw\n",
        "#!lshw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mxRDyesZ6LTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "ad822429-378c-4fdc-9550-a60750da8d81"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import gmtime, strftime\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.python.keras import utils\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tb_home_dir = '/content/drive/My Drive/ML/直感 Deep Learning/drive/'\n",
        "\n",
        "\n",
        "! npm install -g localtunnel\n",
        "!rm url.txt\n",
        "\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(tb_home_dir)\n",
        ")\n",
        "get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
            "+ localtunnel@1.9.1\n",
            "updated 1 package in 1.753s\n",
            "\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[33m   ╭─────────────────────────────────────╮\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                     \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m   Update available \u001b[2m5.7.1\u001b[22m\u001b[0m → \u001b[0m\u001b[32m6.4.1\u001b[39m    \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m     Run \u001b[36mnpm i -g npm\u001b[39m to update      \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                     \u001b[33m│\u001b[39m\n",
            "\u001b[33m   ╰─────────────────────────────────────╯\u001b[39m\n",
            "\u001b[33m\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q6J-xeDr6Qym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fda2266-c935-4a30-d24b-a7ab4c8140c4"
      },
      "cell_type": "code",
      "source": [
        "!cat url.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your url is: https://unlucky-husky-54.localtunnel.me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U17ykJ6g6TXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lenet(input_shape, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=20, kernel_size=5, padding='same', input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Conv2D(filters=50, kernel_size=5, padding='same', input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(500, activation=\"relu\"))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f63Iqfjw8SbP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNISTDataset():\n",
        "  def __init__(self):\n",
        "    self.image_shape=(28, 28, 1) # grayscale\n",
        "    self.num_classes = 10\n",
        "    \n",
        "  def get_batch(self):\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "    y_train, y_test = [self.preprocess(d, True) for d in [y_train, y_test]]\n",
        "    return x_train, y_train, x_test, y_test\n",
        "    \n",
        "  def preprocess(self, data, label_data=False):\n",
        "    if label_data: # y label\n",
        "      data = utils.to_categorical(data, self.num_classes)\n",
        "    else:\n",
        "      data = data.astype(\"float32\")\n",
        "      data /= 255\n",
        "      shape = (data.shape[0], ) + self.image_shape # data numの後ろに image shapeをつける\n",
        "      data = data.reshape(shape)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6eZAFujm_L3e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "  def __init__(self, model, loss, optimizer, set_dir_name):\n",
        "    self._target = model\n",
        "    self._target.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    self.verbose=1\n",
        "    self.log_dir = self.get_log_dir(set_dir_name=set_dir_name)\n",
        "\n",
        "    \n",
        "  def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "    if os.path.exists(self.log_dir):\n",
        "      import shutil\n",
        "      shutil.rmtree(self.log_dir)\n",
        "    os.mkdir(self.log_dir)\n",
        "    \n",
        "    self._target.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size, epochs=epochs,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
        "        verbose=self.verbose\n",
        "    )\n",
        "  \n",
        "  \n",
        "  def get_log_dir(self, set_dir_name = ''):\n",
        "    \"\"\"return log_dir\"\"\"\n",
        "    tictoc = strftime('%a_%d_%b_%Y_%H_%M_%S', gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name +'_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    return log_dir\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdrsxqQrIlG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST"
      ]
    },
    {
      "metadata": {
        "id": "WObU8-Ni_pIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "555f74f1-779d-4db1-a341-bb4377c0af63"
      },
      "cell_type": "code",
      "source": [
        "dataset = MNISTDataset()\n",
        "model = lenet(dataset.image_shape, dataset.num_classes)\n",
        "model.summary()\n",
        "\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), set_dir_name=\"conv_v1\")\n",
        "\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=12, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Train loss:\", score[1])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 20)        520       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 50)        25050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2450)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               1225500   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5010      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,256,080\n",
            "Trainable params: 1,256,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/12\n",
            "48000/48000 [==============================] - 8s 168us/step - loss: 0.1756 - acc: 0.9454 - val_loss: 0.0691 - val_acc: 0.9784\n",
            "Epoch 2/12\n",
            "48000/48000 [==============================] - 5s 108us/step - loss: 0.0455 - acc: 0.9860 - val_loss: 0.0476 - val_acc: 0.9862\n",
            "Epoch 3/12\n",
            "48000/48000 [==============================] - 5s 108us/step - loss: 0.0296 - acc: 0.9909 - val_loss: 0.0349 - val_acc: 0.9897\n",
            "Epoch 4/12\n",
            "48000/48000 [==============================] - 5s 107us/step - loss: 0.0212 - acc: 0.9932 - val_loss: 0.0354 - val_acc: 0.9905\n",
            "Epoch 5/12\n",
            "48000/48000 [==============================] - 5s 108us/step - loss: 0.0160 - acc: 0.9951 - val_loss: 0.0336 - val_acc: 0.9902\n",
            "Epoch 6/12\n",
            "48000/48000 [==============================] - 5s 107us/step - loss: 0.0154 - acc: 0.9946 - val_loss: 0.0342 - val_acc: 0.9904\n",
            "Epoch 7/12\n",
            "48000/48000 [==============================] - 5s 109us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0349 - val_acc: 0.9906\n",
            "Epoch 8/12\n",
            "48000/48000 [==============================] - 5s 108us/step - loss: 0.0082 - acc: 0.9971 - val_loss: 0.0332 - val_acc: 0.9909\n",
            "Epoch 9/12\n",
            "48000/48000 [==============================] - 5s 107us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0350 - val_acc: 0.9902\n",
            "Epoch 10/12\n",
            "48000/48000 [==============================] - 5s 108us/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0534 - val_acc: 0.9880\n",
            "Epoch 11/12\n",
            "48000/48000 [==============================] - 5s 107us/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0416 - val_acc: 0.9909\n",
            "Epoch 12/12\n",
            "48000/48000 [==============================] - 5s 108us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0390 - val_acc: 0.9905\n",
            "Test loss: 0.030035809845007543\n",
            "Train loss: 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jZEm3ZvjIm-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10"
      ]
    },
    {
      "metadata": {
        "id": "XINS033_EsKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "class CIFAR10Dataset():\n",
        "  def __init__(self):\n",
        "    self.image_shape=(32, 32, 3) # grayscale\n",
        "    self.num_classes = 10\n",
        "    \n",
        "  def get_batch(self):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "    y_train, y_test = [self.preprocess(d, True) for d in [y_train, y_test]]\n",
        "    return x_train, y_train, x_test, y_test\n",
        "    \n",
        "  def preprocess(self, data, label_data=False):\n",
        "    if label_data: # y label\n",
        "      data = utils.to_categorical(data, self.num_classes)\n",
        "    else:\n",
        "      data = data.astype(\"float32\")\n",
        "      data /= 255\n",
        "      shape = (data.shape[0], ) + self.image_shape # data numの後ろに image shapeをつける\n",
        "      data = data.reshape(shape)\n",
        "    return data\n",
        "  \n",
        "class Trainer():\n",
        "  def __init__(self, model, loss, optimizer, set_dir_name):\n",
        "    self._target = model\n",
        "    self._target.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    self.verbose=1\n",
        "    self.log_dir = self.get_log_dir(set_dir_name=set_dir_name)\n",
        "    self.model_file_name = \"model_file.hdf5\"\n",
        "\n",
        "\n",
        "  def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "    if os.path.exists(self.log_dir):\n",
        "      import shutil\n",
        "      shutil.rmtree(self.log_dir)\n",
        "    os.mkdir(self.log_dir)\n",
        "\n",
        "    self._target.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size, epochs=epochs,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[TensorBoard(log_dir=self.log_dir),\n",
        "                  ModelCheckpoint(os.path.join(self.log_dir, self.model_file_name), save_best_only=True)],\n",
        "        verbose=self.verbose\n",
        "    )\n",
        "\n",
        "\n",
        "  def get_log_dir(self, set_dir_name = ''):\n",
        "    \"\"\"return log_dir\"\"\"\n",
        "    tictoc = strftime('%a_%d_%b_%Y_%H_%M_%S', gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name +'_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    return log_dir\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SRdqrVpKJtou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "7b6ceacf-4430-4d81-b436-87008877ff5f"
      },
      "cell_type": "code",
      "source": [
        "dataset = CIFAR10Dataset()\n",
        "model = lenet(dataset.image_shape, dataset.num_classes)\n",
        "model.summary()\n",
        "\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), set_dir_name=\"conv_v2\")\n",
        "\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=12, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Train loss:\", score[1])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 20)        1520      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 50)        25050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 500)               1600500   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5010      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,632,080\n",
            "Trainable params: 1,632,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "40000/40000 [==============================] - 6s 153us/step - loss: 1.5499 - acc: 0.4430 - val_loss: 1.2666 - val_acc: 0.5521\n",
            "Epoch 2/12\n",
            "40000/40000 [==============================] - 6s 140us/step - loss: 1.1515 - acc: 0.5934 - val_loss: 1.0496 - val_acc: 0.6348\n",
            "Epoch 3/12\n",
            "40000/40000 [==============================] - 6s 139us/step - loss: 0.9821 - acc: 0.6570 - val_loss: 1.0632 - val_acc: 0.6297\n",
            "Epoch 4/12\n",
            "40000/40000 [==============================] - 6s 140us/step - loss: 0.8563 - acc: 0.7020 - val_loss: 0.9375 - val_acc: 0.6757\n",
            "Epoch 5/12\n",
            "40000/40000 [==============================] - 6s 139us/step - loss: 0.7485 - acc: 0.7411 - val_loss: 0.9065 - val_acc: 0.6887\n",
            "Epoch 6/12\n",
            "40000/40000 [==============================] - 6s 138us/step - loss: 0.6532 - acc: 0.7711 - val_loss: 0.9453 - val_acc: 0.6868\n",
            "Epoch 7/12\n",
            "40000/40000 [==============================] - 6s 138us/step - loss: 0.5473 - acc: 0.8113 - val_loss: 0.9198 - val_acc: 0.6979\n",
            "Epoch 8/12\n",
            "40000/40000 [==============================] - 6s 138us/step - loss: 0.4528 - acc: 0.8434 - val_loss: 0.9523 - val_acc: 0.6974\n",
            "Epoch 9/12\n",
            "40000/40000 [==============================] - 6s 138us/step - loss: 0.3560 - acc: 0.8777 - val_loss: 1.0044 - val_acc: 0.7002\n",
            "Epoch 10/12\n",
            "40000/40000 [==============================] - 5s 137us/step - loss: 0.2681 - acc: 0.9112 - val_loss: 1.1125 - val_acc: 0.6932\n",
            "Epoch 11/12\n",
            "40000/40000 [==============================] - 5s 137us/step - loss: 0.1941 - acc: 0.9357 - val_loss: 1.1442 - val_acc: 0.6986\n",
            "Epoch 12/12\n",
            "40000/40000 [==============================] - 6s 138us/step - loss: 0.1375 - acc: 0.9571 - val_loss: 1.2591 - val_acc: 0.7042\n",
            "Test loss: 1.3270775800704957\n",
            "Train loss: 0.6879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DwK1_TbZKo-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def network(input_shape, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(filters=64, kernel_size=5, padding='same', input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(filters=64, kernel_size=5, input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gdt_CgMuJm55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1133
        },
        "outputId": "a2e21d97-f774-423a-ec4e-6114fb227983"
      },
      "cell_type": "code",
      "source": [
        "dataset = CIFAR10Dataset()\n",
        "model = network(dataset.image_shape, dataset.num_classes)\n",
        "model.summary()\n",
        "\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), set_dir_name=\"conv_v2\")\n",
        "\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=12, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Train loss:\", score[1])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 4, 4, 64)          102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 291,338\n",
            "Trainable params: 291,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "40000/40000 [==============================] - 7s 183us/step - loss: 1.7896 - acc: 0.3267 - val_loss: 1.4521 - val_acc: 0.4800\n",
            "Epoch 2/12\n",
            "40000/40000 [==============================] - 7s 163us/step - loss: 1.4033 - acc: 0.4883 - val_loss: 1.2609 - val_acc: 0.5449\n",
            "Epoch 3/12\n",
            "40000/40000 [==============================] - 7s 164us/step - loss: 1.2461 - acc: 0.5519 - val_loss: 1.0898 - val_acc: 0.6110\n",
            "Epoch 4/12\n",
            "40000/40000 [==============================] - 7s 165us/step - loss: 1.1477 - acc: 0.5889 - val_loss: 1.0308 - val_acc: 0.6326\n",
            "Epoch 5/12\n",
            "40000/40000 [==============================] - 7s 164us/step - loss: 1.0739 - acc: 0.6203 - val_loss: 0.9507 - val_acc: 0.6658\n",
            "Epoch 6/12\n",
            "40000/40000 [==============================] - 7s 164us/step - loss: 1.0157 - acc: 0.6423 - val_loss: 0.9109 - val_acc: 0.6746\n",
            "Epoch 7/12\n",
            "40000/40000 [==============================] - 6s 162us/step - loss: 0.9700 - acc: 0.6583 - val_loss: 0.9177 - val_acc: 0.6779\n",
            "Epoch 8/12\n",
            "40000/40000 [==============================] - 6s 162us/step - loss: 0.9313 - acc: 0.6743 - val_loss: 0.8405 - val_acc: 0.7054\n",
            "Epoch 9/12\n",
            "40000/40000 [==============================] - 6s 162us/step - loss: 0.9020 - acc: 0.6862 - val_loss: 0.8101 - val_acc: 0.7145\n",
            "Epoch 10/12\n",
            "40000/40000 [==============================] - 6s 161us/step - loss: 0.8764 - acc: 0.6922 - val_loss: 0.7976 - val_acc: 0.7203\n",
            "Epoch 11/12\n",
            "40000/40000 [==============================] - 6s 162us/step - loss: 0.8489 - acc: 0.6993 - val_loss: 0.8171 - val_acc: 0.7110\n",
            "Epoch 12/12\n",
            "40000/40000 [==============================] - 7s 164us/step - loss: 0.8265 - acc: 0.7119 - val_loss: 0.7629 - val_acc: 0.7341\n",
            "Test loss: 0.7756543720245361\n",
            "Train loss: 0.7284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CDVTZ0L9L-fu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "lNIE_vqVL9wJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UY1z6yr3MNTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class Trainer():\n",
        "  def __init__(self, model, loss, optimizer, set_dir_name):\n",
        "    self._target = model\n",
        "    self._target.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    self.verbose=1\n",
        "    self.log_dir = self.get_log_dir(set_dir_name=set_dir_name)\n",
        "    self.model_file_name = \"model_file.hdf5\"\n",
        "\n",
        "\n",
        "  def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "    if os.path.exists(self.log_dir):\n",
        "      import shutil\n",
        "      shutil.rmtree(self.log_dir)\n",
        "    os.mkdir(self.log_dir)\n",
        "    \n",
        "    datagen = ImageDataGenerator(\n",
        "      featurewise_center=False,\n",
        "      samplewise_center=False, \n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      zca_whitening=False, \n",
        "      rotation_range=0, \n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False)\n",
        "    \n",
        "    print(x_train.shape)\n",
        "    datagen.fit(x_train)\n",
        "    print(x_train.shape)\n",
        "    \n",
        "    indices = np.arange(x_train.shape[0]) # data num\n",
        "    np.random.shuffle(indices)\n",
        "    validation_size = int(x_train.shape[0]*validation_split)\n",
        "    x_train, x_valid = x_train[indices[:-validation_size],:], x_train[indices[-validation_size:],:]\n",
        "    y_train, y_valid = y_train[indices[:-validation_size],:], y_train[indices[-validation_size:],:]\n",
        "    \n",
        "\n",
        "    print(self.log_dir)\n",
        "    print(os.path.join(self.log_dir, self.model_file_name))\n",
        "    self._target.fit_generator(\n",
        "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_valid, y_valid),\n",
        "        callbacks=[TensorBoard(log_dir=self.log_dir),\n",
        "                  ModelCheckpoint(os.path.join(self.log_dir, self.model_file_name), save_best_only=True)],\n",
        "        verbose=self.verbose,\n",
        "        workers=4\n",
        "        \n",
        "    )\n",
        "\n",
        "  def get_log_dir(self, set_dir_name = ''):\n",
        "    \"\"\"return log_dir\"\"\"\n",
        "    tictoc = strftime('%a_%d_%b_%Y_%H_%M_%S', gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = os.path.join(tb_home_dir, set_dir_name +'_' + directory_name)\n",
        "    os.mkdir(log_dir)\n",
        "    return log_dir\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_qiw5LmO4Ah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "b8385f9c-874f-4f37-dd85-421ff893a828"
      },
      "cell_type": "code",
      "source": [
        "dataset = CIFAR10Dataset()\n",
        "model = network(dataset.image_shape, dataset.num_classes)\n",
        "#model.summary()\n",
        "\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), set_dir_name=\"conv_v2\")\n",
        "\n",
        "trainer.train(x_train, y_train, batch_size=128, epochs=3, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Train loss:\", score[1])\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 32, 32, 3)\n",
            "/content/drive/My Drive/ML/直感 Deep Learning/drive/conv_v2_Sat_03_Nov_2018_11_15_05\n",
            "/content/drive/My Drive/ML/直感 Deep Learning/drive/conv_v2_Sat_03_Nov_2018_11_15_05/model_file.hdf5\n",
            "Epoch 1/3\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.8475 - acc: 0.3095 - val_loss: 1.4662 - val_acc: 0.4610\n",
            "Epoch 2/3\n",
            "312/312 [==============================] - 29s 95ms/step - loss: 1.4624 - acc: 0.4632 - val_loss: 1.2516 - val_acc: 0.5442\n",
            "Epoch 3/3\n",
            "312/312 [==============================] - 29s 94ms/step - loss: 1.3401 - acc: 0.5167 - val_loss: 1.1183 - val_acc: 0.6007\n",
            "Test loss: 1.1177152481079102\n",
            "Train loss: 0.5981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ir36qJ6FTk6l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 再利用"
      ]
    },
    {
      "metadata": {
        "id": "MgapbtFWPBL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "5bb16a1e-b528-4fcb-fb26-56b1ee6527ba"
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "tb_home_dir = '/content/drive/My Drive/ML/直感 Deep Learning/drive/'\n",
        "\n",
        "model_path = os.path.join(tb_home_dir, 'conv_v2','model_file.hdf5')\n",
        "\n",
        "model = load_model(model_path)\n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_37 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 16, 16, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 4, 4, 64)          102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 291,338\n",
            "Trainable params: 291,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JquOT9g2UNwL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}